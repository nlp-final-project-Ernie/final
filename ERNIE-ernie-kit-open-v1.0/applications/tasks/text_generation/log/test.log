INFO: 12-16 14:49:10: params.py:43 * 12524 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-16 14:49:10: params.py:52 * 12524 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
ERROR: 12-16 14:49:10: register.py:106 * 12524 error in import modules
ERROR: 12-16 14:49:10: register.py:107 * 12524 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\doie_ernie_tiny_tokenizer.py", line 18, in <module>
    import sentencepiece as sp
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-16 14:49:10: register.py:106 * 12524 error in import modules
ERROR: 12-16 14:49:10: register.py:107 * 12524 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\mrc_tokenizer.py", line 21, in <module>
    import sentencepiece as sp
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-16 14:49:10: register.py:106 * 12524 error in import modules
ERROR: 12-16 14:49:10: register.py:107 * 12524 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\tokenization_erniem.py", line 34, in <module>
    import sentencepiece as spm
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-16 14:49:10: register.py:106 * 12524 error in import modules
ERROR: 12-16 14:49:10: register.py:107 * 12524 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\tokenization_mix.py", line 31, in <module>
    from .tokenization_wp import FullTokenizer
  File "../../..\erniekit\data\tokenizer\tokenization_wp.py", line 32, in <module>
    import sentencepiece as sp
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-16 14:49:10: register.py:106 * 12524 error in import modules
ERROR: 12-16 14:49:10: register.py:107 * 12524 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\tokenization_spm.py", line 34, in <module>
    import sentencepiece as spm
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-16 14:49:10: register.py:106 * 12524 error in import modules
ERROR: 12-16 14:49:10: register.py:107 * 12524 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\tokenization_wp.py", line 32, in <module>
    import sentencepiece as sp
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-16 14:49:10: register.py:124 * 12524 error in import ernie_infilling_generation
ERROR: 12-16 14:49:10: register.py:125 * 12524 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 121, in import_new_module
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "D:\Downloads\ERNIE-ernie-kit-open-v1.0_\ERNIE-ernie-kit-open-v1.0\applications\tasks\text_generation\model\ernie_infilling_generation.py", line 21, in <module>
    from erniekit.metrics.gen_eval import GenerationEval
  File "../../..\erniekit\metrics\gen_eval.py", line 25, in <module>
    from erniekit.data.tokenizer.tokenization_wp import BasicTokenizer
  File "../../..\erniekit\data\tokenizer\tokenization_wp.py", line 32, in <module>
    import sentencepiece as sp
ModuleNotFoundError: No module named 'sentencepiece'

INFO: 12-17 01:32:17: params.py:43 * 8116 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 01:32:17: params.py:52 * 8116 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
ERROR: 12-17 01:32:17: register.py:106 * 8116 error in import modules
ERROR: 12-17 01:32:17: register.py:107 * 8116 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\doie_ernie_tiny_tokenizer.py", line 18, in <module>
    import sentencepiece as sp
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-17 01:32:17: register.py:106 * 8116 error in import modules
ERROR: 12-17 01:32:17: register.py:107 * 8116 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\mrc_tokenizer.py", line 21, in <module>
    import sentencepiece as sp
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-17 01:32:17: register.py:106 * 8116 error in import modules
ERROR: 12-17 01:32:17: register.py:107 * 8116 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\tokenization_erniem.py", line 34, in <module>
    import sentencepiece as spm
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-17 01:32:17: register.py:106 * 8116 error in import modules
ERROR: 12-17 01:32:17: register.py:107 * 8116 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\tokenization_mix.py", line 31, in <module>
    from .tokenization_wp import FullTokenizer
  File "../../..\erniekit\data\tokenizer\tokenization_wp.py", line 32, in <module>
    import sentencepiece as sp
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-17 01:32:17: register.py:106 * 8116 error in import modules
ERROR: 12-17 01:32:17: register.py:107 * 8116 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\tokenization_spm.py", line 34, in <module>
    import sentencepiece as spm
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-17 01:32:17: register.py:106 * 8116 error in import modules
ERROR: 12-17 01:32:17: register.py:107 * 8116 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 103, in import_modules
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "../../..\erniekit\data\tokenizer\tokenization_wp.py", line 32, in <module>
    import sentencepiece as sp
ModuleNotFoundError: No module named 'sentencepiece'

ERROR: 12-17 01:32:17: register.py:124 * 8116 error in import ernie_infilling_generation
ERROR: 12-17 01:32:17: register.py:125 * 8116 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 121, in import_new_module
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "D:\Downloads\ERNIE-ernie-kit-open-v1.0_\ERNIE-ernie-kit-open-v1.0\applications\tasks\text_generation\model\ernie_infilling_generation.py", line 21, in <module>
    from erniekit.metrics.gen_eval import GenerationEval
  File "../../..\erniekit\metrics\gen_eval.py", line 25, in <module>
    from erniekit.data.tokenizer.tokenization_wp import BasicTokenizer
  File "../../..\erniekit\data\tokenizer\tokenization_wp.py", line 32, in <module>
    import sentencepiece as sp
ModuleNotFoundError: No module named 'sentencepiece'

INFO: 12-17 01:36:21: params.py:43 * 16112 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 01:36:21: params.py:52 * 16112 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 01:36:21: register.py:25 * 16112 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 01:42:54: params.py:43 * 4624 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 01:42:54: params.py:52 * 4624 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 01:42:54: register.py:25 * 4624 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 01:42:54: run_trainer_ernie_gen.py:91 * 4624 run trainer.... pid = 3152
INFO: 12-17 01:43:47: params.py:43 * 14272 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 01:43:47: params.py:52 * 14272 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 01:43:47: register.py:25 * 14272 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 01:43:47: run_trainer_ernie_gen.py:91 * 14272 run trainer.... pid = 4004
INFO: 12-17 01:44:51: params.py:43 * 14088 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 01:44:51: params.py:52 * 14088 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 01:44:51: register.py:25 * 14088 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 01:44:51: run_trainer_ernie_gen.py:91 * 14088 run trainer.... pid = 19332
INFO: 12-17 02:08:08: params.py:43 * 16772 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 02:08:08: params.py:52 * 16772 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 02:08:08: register.py:25 * 16772 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 02:08:08: run_trainer_ernie_gen.py:91 * 16772 run trainer.... pid = 16476
INFO: 12-17 02:14:52: params.py:43 * 19404 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 02:14:52: params.py:52 * 19404 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 02:14:52: register.py:25 * 19404 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 02:14:52: run_trainer_ernie_gen.py:91 * 19404 run trainer.... pid = 19304
INFO: 12-17 02:14:52: run_trainer_ernie_gen.py:59 * 19404 Device count: 1
INFO: 12-17 02:14:52: run_trainer_ernie_gen.py:60 * 19404 Num train examples: 15763
INFO: 12-17 02:14:52: run_trainer_ernie_gen.py:61 * 19404 Max train steps: 39407
INFO: 12-17 02:14:52: run_trainer_ernie_gen.py:62 * 19404 Num warmup steps: 3940
INFO: 12-17 02:14:52: ernie_infilling_generation.py:31 * 19404 ErnieInfillingGeneration init....
INFO: 12-17 02:14:52: static_trainer_ernie_gen.py:89 * 19404 parser meta ....
INFO: 12-17 02:14:52: static_trainer_ernie_gen.py:542 * 19404 pre_train_model's name = ernie_gen_ch
INFO: 12-17 02:14:52: static_trainer_ernie_gen.py:119 * 19404 init environment on static mode......
INFO: 12-17 02:14:52: static_trainer_ernie_gen.py:163 * 19404 gpu place....
INFO: 12-17 13:21:57: params.py:43 * 23024 ./examples/cls_ernie_gen_infilling_ch_infer.json
INFO: 12-17 13:21:57: params.py:52 * 23024 {
    "dataset_reader": {
        "predict_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/test.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": false,
                "need_generate_examples": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "predict_reader",
            "type": "InfillingGenReader"
        }
    },
    "inference": {
        "PADDLE_PLACE_TYPE": "gpu",
        "extra_param": {
            "meta": {
                "job_type": "text_generation"
            }
        },
        "inference_model_path": "output/ernie_gen_ch/save_inference_model/inference_step_39421/",
        "output_path": "./output/predict_result.txt",
        "thread_num": 1,
        "type": "CustomInference"
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    }
}
WARNING: 12-17 13:21:57: register.py:25 * 23024 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 13:21:57: ernie_infilling_generation.py:31 * 23024 ErnieInfillingGeneration init....
INFO: 12-17 13:21:57: params.py:43 * 23024 output/ernie_gen_ch/save_inference_model/inference_step_39421/infer_data_params.json
INFO: 12-17 17:10:21: params.py:43 * 22428 ./examples/cls_ernie_gen_infilling_ch_infer.json
INFO: 12-17 17:10:21: params.py:52 * 22428 {
    "dataset_reader": {
        "predict_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/test.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": false,
                "need_generate_examples": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "predict_reader",
            "type": "InfillingGenReader"
        }
    },
    "inference": {
        "PADDLE_PLACE_TYPE": "gpu",
        "extra_param": {
            "meta": {
                "job_type": "text_generation"
            }
        },
        "inference_model_path": "output/ernie_gen_ch/save_inference_model/inference_step_39421/",
        "output_path": "./output/predict_result.txt",
        "thread_num": 1,
        "type": "CustomInference"
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    }
}
WARNING: 12-17 17:10:21: register.py:25 * 22428 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 17:10:21: ernie_infilling_generation.py:31 * 22428 ErnieInfillingGeneration init....
INFO: 12-17 17:10:21: params.py:43 * 22428 output/ernie_gen_ch/save_inference_model/inference_step_39421/infer_data_params.json
INFO: 12-17 17:11:10: params.py:43 * 6960 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 17:11:10: params.py:52 * 6960 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 17:11:10: register.py:25 * 6960 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 17:11:11: run_trainer_ernie_gen.py:91 * 6960 run trainer.... pid = 23652
INFO: 12-17 17:11:11: run_trainer_ernie_gen.py:59 * 6960 Device count: 1
INFO: 12-17 17:11:11: run_trainer_ernie_gen.py:60 * 6960 Num train examples: 15763
INFO: 12-17 17:11:11: run_trainer_ernie_gen.py:61 * 6960 Max train steps: 39407
INFO: 12-17 17:11:11: run_trainer_ernie_gen.py:62 * 6960 Num warmup steps: 3940
INFO: 12-17 17:11:11: ernie_infilling_generation.py:31 * 6960 ErnieInfillingGeneration init....
INFO: 12-17 17:11:11: static_trainer_ernie_gen.py:89 * 6960 parser meta ....
INFO: 12-17 17:11:11: static_trainer_ernie_gen.py:542 * 6960 pre_train_model's name = ernie_gen_ch
INFO: 12-17 17:11:11: static_trainer_ernie_gen.py:119 * 6960 init environment on static mode......
INFO: 12-17 17:11:11: static_trainer_ernie_gen.py:163 * 6960 gpu place....
INFO: 12-17 17:11:11: static_trainer_ernie_gen.py:344 * 6960 init_model_net.....
INFO: 12-17 17:17:54: params.py:43 * 18136 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 17:17:54: params.py:52 * 18136 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 17:17:54: register.py:25 * 18136 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 17:17:54: run_trainer_ernie_gen.py:91 * 18136 run trainer.... pid = 21348
INFO: 12-17 17:17:54: run_trainer_ernie_gen.py:59 * 18136 Device count: 1
INFO: 12-17 17:17:54: run_trainer_ernie_gen.py:60 * 18136 Num train examples: 15763
INFO: 12-17 17:17:54: run_trainer_ernie_gen.py:61 * 18136 Max train steps: 39407
INFO: 12-17 17:17:54: run_trainer_ernie_gen.py:62 * 18136 Num warmup steps: 3940
INFO: 12-17 17:17:54: ernie_infilling_generation.py:31 * 18136 ErnieInfillingGeneration init....
INFO: 12-17 17:17:54: static_trainer_ernie_gen.py:89 * 18136 parser meta ....
INFO: 12-17 17:17:54: static_trainer_ernie_gen.py:542 * 18136 pre_train_model's name = ernie_gen_ch
INFO: 12-17 17:17:54: static_trainer_ernie_gen.py:119 * 18136 init environment on static mode......
INFO: 12-17 17:17:54: static_trainer_ernie_gen.py:163 * 18136 gpu place....
INFO: 12-17 17:17:54: static_trainer_ernie_gen.py:344 * 18136 init_model_net.....
INFO: 12-17 17:18:43: params.py:43 * 20328 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 17:18:43: params.py:52 * 20328 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 17:18:43: register.py:25 * 20328 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 17:18:43: run_trainer_ernie_gen.py:91 * 20328 run trainer.... pid = 22036
INFO: 12-17 17:18:43: run_trainer_ernie_gen.py:59 * 20328 Device count: 1
INFO: 12-17 17:18:43: run_trainer_ernie_gen.py:60 * 20328 Num train examples: 15763
INFO: 12-17 17:18:43: run_trainer_ernie_gen.py:61 * 20328 Max train steps: 39407
INFO: 12-17 17:18:43: run_trainer_ernie_gen.py:62 * 20328 Num warmup steps: 3940
INFO: 12-17 17:18:43: ernie_infilling_generation.py:31 * 20328 ErnieInfillingGeneration init....
INFO: 12-17 17:18:43: static_trainer_ernie_gen.py:89 * 20328 parser meta ....
INFO: 12-17 17:18:43: static_trainer_ernie_gen.py:542 * 20328 pre_train_model's name = ernie_gen_ch
INFO: 12-17 17:18:43: static_trainer_ernie_gen.py:119 * 20328 init environment on static mode......
INFO: 12-17 17:18:43: static_trainer_ernie_gen.py:163 * 20328 gpu place....
INFO: 12-17 17:18:43: static_trainer_ernie_gen.py:344 * 20328 init_model_net.....
INFO: 12-17 17:18:58: params.py:43 * 16520 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 17:18:58: params.py:52 * 16520 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 17:18:58: register.py:25 * 16520 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 17:18:58: run_trainer_ernie_gen.py:91 * 16520 run trainer.... pid = 18248
INFO: 12-17 17:18:59: run_trainer_ernie_gen.py:59 * 16520 Device count: 1
INFO: 12-17 17:18:59: run_trainer_ernie_gen.py:60 * 16520 Num train examples: 15763
INFO: 12-17 17:18:59: run_trainer_ernie_gen.py:61 * 16520 Max train steps: 39407
INFO: 12-17 17:18:59: run_trainer_ernie_gen.py:62 * 16520 Num warmup steps: 3940
INFO: 12-17 17:18:59: ernie_infilling_generation.py:31 * 16520 ErnieInfillingGeneration init....
INFO: 12-17 17:18:59: static_trainer_ernie_gen.py:89 * 16520 parser meta ....
INFO: 12-17 17:18:59: static_trainer_ernie_gen.py:542 * 16520 pre_train_model's name = ernie_gen_ch
INFO: 12-17 17:18:59: static_trainer_ernie_gen.py:119 * 16520 init environment on static mode......
INFO: 12-17 17:18:59: static_trainer_ernie_gen.py:163 * 16520 gpu place....
INFO: 12-17 17:18:59: static_trainer_ernie_gen.py:344 * 16520 init_model_net.....
INFO: 12-17 17:20:38: params.py:43 * 24312 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 17:20:38: params.py:52 * 24312 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 17:20:38: register.py:25 * 24312 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 17:20:38: run_trainer_ernie_gen.py:91 * 24312 run trainer.... pid = 16184
INFO: 12-17 17:20:38: run_trainer_ernie_gen.py:59 * 24312 Device count: 1
INFO: 12-17 17:20:38: run_trainer_ernie_gen.py:60 * 24312 Num train examples: 15763
INFO: 12-17 17:20:38: run_trainer_ernie_gen.py:61 * 24312 Max train steps: 39407
INFO: 12-17 17:20:38: run_trainer_ernie_gen.py:62 * 24312 Num warmup steps: 3940
INFO: 12-17 17:20:38: ernie_infilling_generation.py:31 * 24312 ErnieInfillingGeneration init....
INFO: 12-17 17:20:38: static_trainer_ernie_gen.py:89 * 24312 parser meta ....
INFO: 12-17 17:20:38: static_trainer_ernie_gen.py:542 * 24312 pre_train_model's name = ernie_gen_ch
INFO: 12-17 17:20:38: static_trainer_ernie_gen.py:119 * 24312 init environment on static mode......
INFO: 12-17 17:20:38: static_trainer_ernie_gen.py:163 * 24312 gpu place....
INFO: 12-17 17:20:38: static_trainer_ernie_gen.py:344 * 24312 init_model_net.....
INFO: 12-17 17:21:35: params.py:43 * 17400 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 17:21:35: params.py:52 * 17400 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 17:21:35: register.py:25 * 17400 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 17:21:35: run_trainer_ernie_gen.py:91 * 17400 run trainer.... pid = 12928
INFO: 12-17 17:21:35: run_trainer_ernie_gen.py:59 * 17400 Device count: 1
INFO: 12-17 17:21:35: run_trainer_ernie_gen.py:60 * 17400 Num train examples: 15763
INFO: 12-17 17:21:35: run_trainer_ernie_gen.py:61 * 17400 Max train steps: 39407
INFO: 12-17 17:21:35: run_trainer_ernie_gen.py:62 * 17400 Num warmup steps: 3940
INFO: 12-17 17:21:35: ernie_infilling_generation.py:31 * 17400 ErnieInfillingGeneration init....
INFO: 12-17 17:21:35: static_trainer_ernie_gen.py:89 * 17400 parser meta ....
INFO: 12-17 17:21:35: static_trainer_ernie_gen.py:542 * 17400 pre_train_model's name = ernie_gen_ch
INFO: 12-17 17:21:35: static_trainer_ernie_gen.py:119 * 17400 init environment on static mode......
INFO: 12-17 17:21:35: static_trainer_ernie_gen.py:163 * 17400 gpu place....
INFO: 12-17 17:21:35: static_trainer_ernie_gen.py:344 * 17400 init_model_net.....
INFO: 12-17 17:23:24: params.py:43 * 24268 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 17:23:24: params.py:52 * 24268 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 17:23:24: register.py:25 * 24268 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 17:23:24: run_trainer_ernie_gen.py:91 * 24268 run trainer.... pid = 10244
INFO: 12-17 17:23:25: run_trainer_ernie_gen.py:59 * 24268 Device count: 1
INFO: 12-17 17:23:25: run_trainer_ernie_gen.py:60 * 24268 Num train examples: 15763
INFO: 12-17 17:23:25: run_trainer_ernie_gen.py:61 * 24268 Max train steps: 39407
INFO: 12-17 17:23:25: run_trainer_ernie_gen.py:62 * 24268 Num warmup steps: 3940
INFO: 12-17 17:23:25: ernie_infilling_generation.py:31 * 24268 ErnieInfillingGeneration init....
INFO: 12-17 17:23:25: static_trainer_ernie_gen.py:89 * 24268 parser meta ....
INFO: 12-17 17:23:25: static_trainer_ernie_gen.py:542 * 24268 pre_train_model's name = ernie_gen_ch
INFO: 12-17 17:23:25: static_trainer_ernie_gen.py:119 * 24268 init environment on static mode......
INFO: 12-17 17:23:25: static_trainer_ernie_gen.py:163 * 24268 gpu place....
INFO: 12-17 17:23:25: static_trainer_ernie_gen.py:344 * 24268 init_model_net.....
INFO: 12-17 17:30:08: params.py:43 * 23580 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 17:30:08: params.py:52 * 23580 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 17:30:08: register.py:25 * 23580 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 17:30:08: run_trainer_ernie_gen.py:91 * 23580 run trainer.... pid = 15260
INFO: 12-17 17:30:08: run_trainer_ernie_gen.py:59 * 23580 Device count: 1
INFO: 12-17 17:30:08: run_trainer_ernie_gen.py:60 * 23580 Num train examples: 15763
INFO: 12-17 17:30:08: run_trainer_ernie_gen.py:61 * 23580 Max train steps: 39407
INFO: 12-17 17:30:08: run_trainer_ernie_gen.py:62 * 23580 Num warmup steps: 3940
INFO: 12-17 17:30:08: ernie_infilling_generation.py:31 * 23580 ErnieInfillingGeneration init....
INFO: 12-17 17:30:08: static_trainer_ernie_gen.py:89 * 23580 parser meta ....
INFO: 12-17 17:30:08: static_trainer_ernie_gen.py:542 * 23580 pre_train_model's name = ernie_gen_ch
INFO: 12-17 17:30:08: static_trainer_ernie_gen.py:119 * 23580 init environment on static mode......
INFO: 12-17 17:30:08: static_trainer_ernie_gen.py:163 * 23580 gpu place....
INFO: 12-17 17:30:08: static_trainer_ernie_gen.py:344 * 23580 init_model_net.....
DEBUG: 12-17 17:30:08: ernie_gen_infilling_dataset_reader.py:145 * 23580 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 20:44:39: params.py:43 * 23428 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 20:44:39: params.py:52 * 23428 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 20:44:39: register.py:25 * 23428 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 20:44:39: run_trainer_ernie_gen.py:91 * 23428 run trainer.... pid = 19848
INFO: 12-17 20:44:39: run_trainer_ernie_gen.py:59 * 23428 Device count: 1
INFO: 12-17 20:44:39: run_trainer_ernie_gen.py:60 * 23428 Num train examples: 15763
INFO: 12-17 20:44:39: run_trainer_ernie_gen.py:61 * 23428 Max train steps: 39407
INFO: 12-17 20:44:39: run_trainer_ernie_gen.py:62 * 23428 Num warmup steps: 3940
INFO: 12-17 20:44:39: ernie_infilling_generation.py:31 * 23428 ErnieInfillingGeneration init....
INFO: 12-17 20:44:39: static_trainer_ernie_gen.py:89 * 23428 parser meta ....
INFO: 12-17 20:44:39: static_trainer_ernie_gen.py:542 * 23428 pre_train_model's name = ernie_gen_ch
INFO: 12-17 20:44:39: static_trainer_ernie_gen.py:119 * 23428 init environment on static mode......
INFO: 12-17 20:44:39: static_trainer_ernie_gen.py:163 * 23428 gpu place....
INFO: 12-17 20:44:39: static_trainer_ernie_gen.py:344 * 23428 init_model_net.....
DEBUG: 12-17 20:44:39: ernie_gen_infilling_dataset_reader.py:145 * 23428 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 20:52:27: params.py:43 * 20840 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 20:52:27: params.py:52 * 20840 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 20:52:27: register.py:25 * 20840 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 20:52:27: run_trainer_ernie_gen.py:91 * 20840 run trainer.... pid = 5808
INFO: 12-17 20:52:27: run_trainer_ernie_gen.py:59 * 20840 Device count: 1
INFO: 12-17 20:52:27: run_trainer_ernie_gen.py:60 * 20840 Num train examples: 15763
INFO: 12-17 20:52:27: run_trainer_ernie_gen.py:61 * 20840 Max train steps: 39407
INFO: 12-17 20:52:27: run_trainer_ernie_gen.py:62 * 20840 Num warmup steps: 3940
INFO: 12-17 20:52:27: ernie_infilling_generation.py:31 * 20840 ErnieInfillingGeneration init....
INFO: 12-17 20:52:27: static_trainer_ernie_gen.py:89 * 20840 parser meta ....
INFO: 12-17 20:52:27: static_trainer_ernie_gen.py:542 * 20840 pre_train_model's name = ernie_gen_ch
INFO: 12-17 20:52:27: static_trainer_ernie_gen.py:119 * 20840 init environment on static mode......
INFO: 12-17 20:52:27: static_trainer_ernie_gen.py:163 * 20840 gpu place....
INFO: 12-17 20:52:27: static_trainer_ernie_gen.py:344 * 20840 init_model_net.....
DEBUG: 12-17 20:52:27: ernie_gen_infilling_dataset_reader.py:145 * 20840 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 21:11:16: params.py:43 * 22868 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 21:11:16: params.py:52 * 22868 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 21:11:16: register.py:25 * 22868 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 21:11:16: run_trainer_ernie_gen.py:91 * 22868 run trainer.... pid = 16804
INFO: 12-17 21:11:16: run_trainer_ernie_gen.py:59 * 22868 Device count: 1
INFO: 12-17 21:11:16: run_trainer_ernie_gen.py:60 * 22868 Num train examples: 15763
INFO: 12-17 21:11:16: run_trainer_ernie_gen.py:61 * 22868 Max train steps: 39407
INFO: 12-17 21:11:16: run_trainer_ernie_gen.py:62 * 22868 Num warmup steps: 3940
INFO: 12-17 21:11:16: ernie_infilling_generation.py:31 * 22868 ErnieInfillingGeneration init....
INFO: 12-17 21:11:16: static_trainer_ernie_gen.py:89 * 22868 parser meta ....
INFO: 12-17 21:11:16: static_trainer_ernie_gen.py:542 * 22868 pre_train_model's name = ernie_gen_ch
INFO: 12-17 21:11:16: static_trainer_ernie_gen.py:119 * 22868 init environment on static mode......
INFO: 12-17 21:11:16: static_trainer_ernie_gen.py:163 * 22868 gpu place....
INFO: 12-17 21:11:16: static_trainer_ernie_gen.py:344 * 22868 init_model_net.....
DEBUG: 12-17 21:11:16: ernie_gen_infilling_dataset_reader.py:145 * 22868 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 21:14:37: params.py:43 * 20072 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 21:14:37: params.py:52 * 20072 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 21:14:37: register.py:25 * 20072 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 21:14:37: run_trainer_ernie_gen.py:91 * 20072 run trainer.... pid = 16576
INFO: 12-17 21:14:37: run_trainer_ernie_gen.py:59 * 20072 Device count: 1
INFO: 12-17 21:14:37: run_trainer_ernie_gen.py:60 * 20072 Num train examples: 15763
INFO: 12-17 21:14:37: run_trainer_ernie_gen.py:61 * 20072 Max train steps: 39407
INFO: 12-17 21:14:37: run_trainer_ernie_gen.py:62 * 20072 Num warmup steps: 3940
INFO: 12-17 21:14:37: ernie_infilling_generation.py:31 * 20072 ErnieInfillingGeneration init....
INFO: 12-17 21:14:37: static_trainer_ernie_gen.py:89 * 20072 parser meta ....
INFO: 12-17 21:14:37: static_trainer_ernie_gen.py:542 * 20072 pre_train_model's name = ernie_gen_ch
INFO: 12-17 21:14:37: static_trainer_ernie_gen.py:119 * 20072 init environment on static mode......
INFO: 12-17 21:14:37: static_trainer_ernie_gen.py:163 * 20072 gpu place....
INFO: 12-17 21:14:37: static_trainer_ernie_gen.py:344 * 20072 init_model_net.....
DEBUG: 12-17 21:14:37: ernie_gen_infilling_dataset_reader.py:145 * 20072 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 21:15:21: params.py:43 * 24716 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 21:15:21: params.py:52 * 24716 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 21:15:21: register.py:25 * 24716 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 21:15:21: run_trainer_ernie_gen.py:91 * 24716 run trainer.... pid = 11840
INFO: 12-17 21:15:21: run_trainer_ernie_gen.py:59 * 24716 Device count: 1
INFO: 12-17 21:15:21: run_trainer_ernie_gen.py:60 * 24716 Num train examples: 15763
INFO: 12-17 21:15:21: run_trainer_ernie_gen.py:61 * 24716 Max train steps: 39407
INFO: 12-17 21:15:21: run_trainer_ernie_gen.py:62 * 24716 Num warmup steps: 3940
INFO: 12-17 21:15:21: ernie_infilling_generation.py:31 * 24716 ErnieInfillingGeneration init....
INFO: 12-17 21:15:21: static_trainer_ernie_gen.py:89 * 24716 parser meta ....
INFO: 12-17 21:15:21: static_trainer_ernie_gen.py:542 * 24716 pre_train_model's name = ernie_gen_ch
INFO: 12-17 21:15:21: static_trainer_ernie_gen.py:119 * 24716 init environment on static mode......
INFO: 12-17 21:15:21: static_trainer_ernie_gen.py:163 * 24716 gpu place....
INFO: 12-17 21:15:21: static_trainer_ernie_gen.py:344 * 24716 init_model_net.....
DEBUG: 12-17 21:15:21: ernie_gen_infilling_dataset_reader.py:145 * 24716 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 21:23:28: params.py:43 * 25192 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 21:23:28: params.py:52 * 25192 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 21:23:28: register.py:25 * 25192 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 21:23:29: run_trainer_ernie_gen.py:91 * 25192 run trainer.... pid = 16372
INFO: 12-17 21:23:29: run_trainer_ernie_gen.py:59 * 25192 Device count: 1
INFO: 12-17 21:23:29: run_trainer_ernie_gen.py:60 * 25192 Num train examples: 15763
INFO: 12-17 21:23:29: run_trainer_ernie_gen.py:61 * 25192 Max train steps: 39407
INFO: 12-17 21:23:29: run_trainer_ernie_gen.py:62 * 25192 Num warmup steps: 3940
INFO: 12-17 21:23:29: ernie_infilling_generation.py:31 * 25192 ErnieInfillingGeneration init....
INFO: 12-17 21:23:29: static_trainer_ernie_gen.py:89 * 25192 parser meta ....
INFO: 12-17 21:23:29: static_trainer_ernie_gen.py:542 * 25192 pre_train_model's name = ernie_gen_ch
INFO: 12-17 21:23:29: static_trainer_ernie_gen.py:119 * 25192 init environment on static mode......
INFO: 12-17 21:23:29: static_trainer_ernie_gen.py:163 * 25192 gpu place....
INFO: 12-17 21:23:29: static_trainer_ernie_gen.py:344 * 25192 init_model_net.....
DEBUG: 12-17 21:23:29: ernie_gen_infilling_dataset_reader.py:145 * 25192 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 21:26:17: params.py:43 * 19420 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 21:26:17: params.py:52 * 19420 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 21:26:17: register.py:25 * 19420 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 21:26:17: run_trainer_ernie_gen.py:91 * 19420 run trainer.... pid = 19592
INFO: 12-17 21:26:18: run_trainer_ernie_gen.py:59 * 19420 Device count: 1
INFO: 12-17 21:26:18: run_trainer_ernie_gen.py:60 * 19420 Num train examples: 15763
INFO: 12-17 21:26:18: run_trainer_ernie_gen.py:61 * 19420 Max train steps: 39407
INFO: 12-17 21:26:18: run_trainer_ernie_gen.py:62 * 19420 Num warmup steps: 3940
INFO: 12-17 21:26:18: ernie_infilling_generation.py:31 * 19420 ErnieInfillingGeneration init....
INFO: 12-17 21:26:18: static_trainer_ernie_gen.py:89 * 19420 parser meta ....
INFO: 12-17 21:26:18: static_trainer_ernie_gen.py:542 * 19420 pre_train_model's name = ernie_gen_ch
INFO: 12-17 21:26:18: static_trainer_ernie_gen.py:119 * 19420 init environment on static mode......
INFO: 12-17 21:26:18: static_trainer_ernie_gen.py:163 * 19420 gpu place....
INFO: 12-17 21:26:18: static_trainer_ernie_gen.py:344 * 19420 init_model_net.....
DEBUG: 12-17 21:26:18: ernie_gen_infilling_dataset_reader.py:145 * 19420 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 21:28:26: params.py:43 * 12800 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 21:28:26: params.py:52 * 12800 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 21:28:26: register.py:25 * 12800 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 21:28:26: run_trainer_ernie_gen.py:91 * 12800 run trainer.... pid = 19928
INFO: 12-17 21:28:26: run_trainer_ernie_gen.py:59 * 12800 Device count: 1
INFO: 12-17 21:28:26: run_trainer_ernie_gen.py:60 * 12800 Num train examples: 15763
INFO: 12-17 21:28:26: run_trainer_ernie_gen.py:61 * 12800 Max train steps: 39407
INFO: 12-17 21:28:26: run_trainer_ernie_gen.py:62 * 12800 Num warmup steps: 3940
INFO: 12-17 21:28:26: ernie_infilling_generation.py:31 * 12800 ErnieInfillingGeneration init....
INFO: 12-17 21:28:26: static_trainer_ernie_gen.py:89 * 12800 parser meta ....
INFO: 12-17 21:28:26: static_trainer_ernie_gen.py:542 * 12800 pre_train_model's name = ernie_gen_ch
INFO: 12-17 21:28:26: static_trainer_ernie_gen.py:119 * 12800 init environment on static mode......
INFO: 12-17 21:28:26: static_trainer_ernie_gen.py:163 * 12800 gpu place....
INFO: 12-17 21:28:26: static_trainer_ernie_gen.py:344 * 12800 init_model_net.....
DEBUG: 12-17 21:28:26: ernie_gen_infilling_dataset_reader.py:145 * 12800 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 21:29:44: params.py:43 * 24508 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 21:29:44: params.py:52 * 24508 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 21:29:44: register.py:25 * 24508 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 21:29:44: run_trainer_ernie_gen.py:91 * 24508 run trainer.... pid = 20732
INFO: 12-17 21:29:45: run_trainer_ernie_gen.py:59 * 24508 Device count: 1
INFO: 12-17 21:29:45: run_trainer_ernie_gen.py:60 * 24508 Num train examples: 15763
INFO: 12-17 21:29:45: run_trainer_ernie_gen.py:61 * 24508 Max train steps: 39407
INFO: 12-17 21:29:45: run_trainer_ernie_gen.py:62 * 24508 Num warmup steps: 3940
INFO: 12-17 21:29:45: ernie_infilling_generation.py:31 * 24508 ErnieInfillingGeneration init....
INFO: 12-17 21:29:45: static_trainer_ernie_gen.py:89 * 24508 parser meta ....
INFO: 12-17 21:29:45: static_trainer_ernie_gen.py:542 * 24508 pre_train_model's name = ernie_gen_ch
INFO: 12-17 21:29:45: static_trainer_ernie_gen.py:119 * 24508 init environment on static mode......
INFO: 12-17 21:29:45: static_trainer_ernie_gen.py:163 * 24508 gpu place....
INFO: 12-17 21:29:45: static_trainer_ernie_gen.py:344 * 24508 init_model_net.....
DEBUG: 12-17 21:29:45: ernie_gen_infilling_dataset_reader.py:145 * 24508 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 21:30:11: params.py:43 * 20240 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 21:30:11: params.py:52 * 20240 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 21:30:11: register.py:25 * 20240 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 21:30:11: run_trainer_ernie_gen.py:91 * 20240 run trainer.... pid = 24856
INFO: 12-17 21:30:12: run_trainer_ernie_gen.py:59 * 20240 Device count: 1
INFO: 12-17 21:30:12: run_trainer_ernie_gen.py:60 * 20240 Num train examples: 15763
INFO: 12-17 21:30:12: run_trainer_ernie_gen.py:61 * 20240 Max train steps: 39407
INFO: 12-17 21:30:12: run_trainer_ernie_gen.py:62 * 20240 Num warmup steps: 3940
INFO: 12-17 21:30:12: ernie_infilling_generation.py:31 * 20240 ErnieInfillingGeneration init....
INFO: 12-17 21:30:12: static_trainer_ernie_gen.py:89 * 20240 parser meta ....
INFO: 12-17 21:30:12: static_trainer_ernie_gen.py:542 * 20240 pre_train_model's name = ernie_gen_ch
INFO: 12-17 21:30:12: static_trainer_ernie_gen.py:119 * 20240 init environment on static mode......
INFO: 12-17 21:30:12: static_trainer_ernie_gen.py:163 * 20240 gpu place....
INFO: 12-17 21:30:12: static_trainer_ernie_gen.py:344 * 20240 init_model_net.....
DEBUG: 12-17 21:30:12: ernie_gen_infilling_dataset_reader.py:145 * 20240 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 21:31:43: params.py:43 * 20312 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 21:31:43: params.py:52 * 20312 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 21:31:43: register.py:25 * 20312 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 21:31:43: run_trainer_ernie_gen.py:91 * 20312 run trainer.... pid = 3572
INFO: 12-17 21:31:43: run_trainer_ernie_gen.py:59 * 20312 Device count: 1
INFO: 12-17 21:31:43: run_trainer_ernie_gen.py:60 * 20312 Num train examples: 15763
INFO: 12-17 21:31:43: run_trainer_ernie_gen.py:61 * 20312 Max train steps: 39407
INFO: 12-17 21:31:43: run_trainer_ernie_gen.py:62 * 20312 Num warmup steps: 3940
INFO: 12-17 21:31:43: ernie_infilling_generation.py:31 * 20312 ErnieInfillingGeneration init....
INFO: 12-17 21:31:43: static_trainer_ernie_gen.py:89 * 20312 parser meta ....
INFO: 12-17 21:31:43: static_trainer_ernie_gen.py:542 * 20312 pre_train_model's name = ernie_gen_ch
INFO: 12-17 21:31:43: static_trainer_ernie_gen.py:119 * 20312 init environment on static mode......
INFO: 12-17 21:31:43: static_trainer_ernie_gen.py:163 * 20312 gpu place....
INFO: 12-17 21:31:43: static_trainer_ernie_gen.py:344 * 20312 init_model_net.....
DEBUG: 12-17 21:31:43: ernie_gen_infilling_dataset_reader.py:145 * 20312 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 22:01:43: params.py:43 * 5088 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:01:43: params.py:52 * 5088 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:01:43: register.py:25 * 5088 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 22:01:43: run_trainer_ernie_gen.py:91 * 5088 run trainer.... pid = 25260
INFO: 12-17 22:01:43: run_trainer_ernie_gen.py:59 * 5088 Device count: 1
INFO: 12-17 22:01:43: run_trainer_ernie_gen.py:60 * 5088 Num train examples: 15763
INFO: 12-17 22:01:43: run_trainer_ernie_gen.py:61 * 5088 Max train steps: 39407
INFO: 12-17 22:01:43: run_trainer_ernie_gen.py:62 * 5088 Num warmup steps: 3940
INFO: 12-17 22:01:43: ernie_infilling_generation.py:31 * 5088 ErnieInfillingGeneration init....
INFO: 12-17 22:01:43: static_trainer_ernie_gen.py:89 * 5088 parser meta ....
INFO: 12-17 22:01:43: static_trainer_ernie_gen.py:542 * 5088 pre_train_model's name = ernie_gen_ch
INFO: 12-17 22:01:43: static_trainer_ernie_gen.py:119 * 5088 init environment on static mode......
INFO: 12-17 22:01:43: static_trainer_ernie_gen.py:163 * 5088 gpu place....
INFO: 12-17 22:01:43: static_trainer_ernie_gen.py:344 * 5088 init_model_net.....
DEBUG: 12-17 22:01:43: ernie_gen_infilling_dataset_reader.py:145 * 5088 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 22:13:56: params.py:43 * 5228 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:13:56: params.py:52 * 5228 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:13:56: register.py:25 * 5228 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 22:13:56: run_trainer_ernie_gen.py:91 * 5228 run trainer.... pid = 8264
INFO: 12-17 22:13:56: run_trainer_ernie_gen.py:59 * 5228 Device count: 1
INFO: 12-17 22:13:56: run_trainer_ernie_gen.py:60 * 5228 Num train examples: 15763
INFO: 12-17 22:13:56: run_trainer_ernie_gen.py:61 * 5228 Max train steps: 39407
INFO: 12-17 22:13:56: run_trainer_ernie_gen.py:62 * 5228 Num warmup steps: 3940
INFO: 12-17 22:13:56: ernie_infilling_generation.py:31 * 5228 ErnieInfillingGeneration init....
INFO: 12-17 22:13:56: static_trainer_ernie_gen.py:89 * 5228 parser meta ....
INFO: 12-17 22:13:56: static_trainer_ernie_gen.py:542 * 5228 pre_train_model's name = ernie_gen_ch
INFO: 12-17 22:13:56: static_trainer_ernie_gen.py:119 * 5228 init environment on static mode......
INFO: 12-17 22:13:56: static_trainer_ernie_gen.py:163 * 5228 gpu place....
INFO: 12-17 22:13:56: static_trainer_ernie_gen.py:344 * 5228 init_model_net.....
DEBUG: 12-17 22:13:56: ernie_gen_infilling_dataset_reader.py:145 * 5228 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 22:16:39: params.py:43 * 22776 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:16:39: params.py:52 * 22776 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:16:39: register.py:25 * 22776 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 22:16:39: run_trainer_ernie_gen.py:91 * 22776 run trainer.... pid = 13712
INFO: 12-17 22:16:39: run_trainer_ernie_gen.py:59 * 22776 Device count: 1
INFO: 12-17 22:16:39: run_trainer_ernie_gen.py:60 * 22776 Num train examples: 15763
INFO: 12-17 22:16:39: run_trainer_ernie_gen.py:61 * 22776 Max train steps: 39407
INFO: 12-17 22:16:39: run_trainer_ernie_gen.py:62 * 22776 Num warmup steps: 3940
INFO: 12-17 22:16:39: ernie_infilling_generation.py:31 * 22776 ErnieInfillingGeneration init....
INFO: 12-17 22:16:39: static_trainer_ernie_gen.py:89 * 22776 parser meta ....
INFO: 12-17 22:16:39: static_trainer_ernie_gen.py:542 * 22776 pre_train_model's name = ernie_gen_ch
INFO: 12-17 22:16:39: static_trainer_ernie_gen.py:119 * 22776 init environment on static mode......
INFO: 12-17 22:16:39: static_trainer_ernie_gen.py:163 * 22776 gpu place....
INFO: 12-17 22:16:39: static_trainer_ernie_gen.py:344 * 22776 init_model_net.....
DEBUG: 12-17 22:16:39: ernie_gen_infilling_dataset_reader.py:145 * 22776 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 22:20:38: params.py:43 * 20264 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:20:38: params.py:52 * 20264 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:20:38: register.py:25 * 20264 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 22:20:38: run_trainer_ernie_gen.py:91 * 20264 run trainer.... pid = 20516
INFO: 12-17 22:20:38: run_trainer_ernie_gen.py:59 * 20264 Device count: 1
INFO: 12-17 22:20:38: run_trainer_ernie_gen.py:60 * 20264 Num train examples: 15763
INFO: 12-17 22:20:38: run_trainer_ernie_gen.py:61 * 20264 Max train steps: 39407
INFO: 12-17 22:20:38: run_trainer_ernie_gen.py:62 * 20264 Num warmup steps: 3940
INFO: 12-17 22:20:38: ernie_infilling_generation.py:31 * 20264 ErnieInfillingGeneration init....
INFO: 12-17 22:20:38: static_trainer_ernie_gen.py:89 * 20264 parser meta ....
INFO: 12-17 22:20:38: static_trainer_ernie_gen.py:542 * 20264 pre_train_model's name = ernie_gen_ch
INFO: 12-17 22:20:38: static_trainer_ernie_gen.py:119 * 20264 init environment on static mode......
INFO: 12-17 22:20:38: static_trainer_ernie_gen.py:163 * 20264 gpu place....
INFO: 12-17 22:20:38: static_trainer_ernie_gen.py:344 * 20264 init_model_net.....
DEBUG: 12-17 22:20:38: ernie_gen_infilling_dataset_reader.py:145 * 20264 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 22:23:37: params.py:43 * 23804 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:23:37: params.py:52 * 23804 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:23:37: register.py:25 * 23804 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 22:23:37: run_trainer_ernie_gen.py:91 * 23804 run trainer.... pid = 24252
INFO: 12-17 22:23:37: run_trainer_ernie_gen.py:59 * 23804 Device count: 1
INFO: 12-17 22:23:37: run_trainer_ernie_gen.py:60 * 23804 Num train examples: 15763
INFO: 12-17 22:23:37: run_trainer_ernie_gen.py:61 * 23804 Max train steps: 39407
INFO: 12-17 22:23:37: run_trainer_ernie_gen.py:62 * 23804 Num warmup steps: 3940
INFO: 12-17 22:23:37: ernie_infilling_generation.py:31 * 23804 ErnieInfillingGeneration init....
INFO: 12-17 22:23:37: static_trainer_ernie_gen.py:89 * 23804 parser meta ....
INFO: 12-17 22:23:37: static_trainer_ernie_gen.py:542 * 23804 pre_train_model's name = ernie_gen_ch
INFO: 12-17 22:23:37: static_trainer_ernie_gen.py:119 * 23804 init environment on static mode......
INFO: 12-17 22:23:37: static_trainer_ernie_gen.py:163 * 23804 gpu place....
INFO: 12-17 22:23:37: static_trainer_ernie_gen.py:344 * 23804 init_model_net.....
DEBUG: 12-17 22:23:37: ernie_gen_infilling_dataset_reader.py:145 * 23804 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 22:27:17: params.py:43 * 20396 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:27:17: params.py:52 * 20396 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:27:17: register.py:25 * 20396 Key WordsegTokenizer already in registry tokenizer.
ERROR: 12-17 22:27:17: register.py:124 * 20396 error in import ernie_infilling_generation
ERROR: 12-17 22:27:17: register.py:125 * 20396 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 121, in import_new_module
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "D:\Downloads\ERNIE-ernie-kit-open-v1.0_\ERNIE-ernie-kit-open-v1.0\applications\tasks\text_generation\model\ernie_infilling_generation.py", line 18, in <module>
    from erniekit.modules.ernie_gen import ErnieGenModel
  File "../../..\erniekit\modules\ernie_gen.py", line 20, in <module>
    from erniekit.modules.transformer_encoder_gen import encoder, two_stream_encoder, pre_process_layer
  File "../../..\erniekit\modules\transformer_encoder_gen.py", line 311
    out = F.dropout(out, p=dropout_rate)
    ^
IndentationError: unexpected indent

INFO: 12-17 22:27:17: run_trainer_ernie_gen.py:91 * 20396 run trainer.... pid = 20148
INFO: 12-17 22:27:17: run_trainer_ernie_gen.py:59 * 20396 Device count: 1
INFO: 12-17 22:27:17: run_trainer_ernie_gen.py:60 * 20396 Num train examples: 15763
INFO: 12-17 22:27:17: run_trainer_ernie_gen.py:61 * 20396 Max train steps: 39407
INFO: 12-17 22:27:17: run_trainer_ernie_gen.py:62 * 20396 Num warmup steps: 3940
ERROR: 12-17 22:27:17: register.py:46 * 20396 module {key} not found: {e}
INFO: 12-17 22:28:01: params.py:43 * 19148 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:28:01: params.py:52 * 19148 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:28:01: register.py:25 * 19148 Key WordsegTokenizer already in registry tokenizer.
ERROR: 12-17 22:28:01: register.py:124 * 19148 error in import ernie_infilling_generation
ERROR: 12-17 22:28:01: register.py:125 * 19148 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 121, in import_new_module
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "D:\Downloads\ERNIE-ernie-kit-open-v1.0_\ERNIE-ernie-kit-open-v1.0\applications\tasks\text_generation\model\ernie_infilling_generation.py", line 18, in <module>
    from erniekit.modules.ernie_gen import ErnieGenModel
  File "../../..\erniekit\modules\ernie_gen.py", line 20, in <module>
    from erniekit.modules.transformer_encoder_gen import encoder, two_stream_encoder, pre_process_layer
  File "../../..\erniekit\modules\transformer_encoder_gen.py", line 311
    out = F.dropout(out, p=dropout_rate)
    ^
IndentationError: unexpected indent

INFO: 12-17 22:28:01: run_trainer_ernie_gen.py:91 * 19148 run trainer.... pid = 5536
INFO: 12-17 22:28:01: run_trainer_ernie_gen.py:59 * 19148 Device count: 1
INFO: 12-17 22:28:01: run_trainer_ernie_gen.py:60 * 19148 Num train examples: 15763
INFO: 12-17 22:28:01: run_trainer_ernie_gen.py:61 * 19148 Max train steps: 39407
INFO: 12-17 22:28:01: run_trainer_ernie_gen.py:62 * 19148 Num warmup steps: 3940
ERROR: 12-17 22:28:01: register.py:46 * 19148 module {key} not found: {e}
INFO: 12-17 22:28:42: params.py:43 * 12396 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:28:42: params.py:52 * 12396 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:28:42: register.py:25 * 12396 Key WordsegTokenizer already in registry tokenizer.
ERROR: 12-17 22:28:42: register.py:124 * 12396 error in import ernie_infilling_generation
ERROR: 12-17 22:28:42: register.py:125 * 12396 traceback.format_exc():
Traceback (most recent call last):
  File "../../..\erniekit\common\register.py", line 121, in import_new_module
    importlib.import_module(full_name)
  File "C:\Users\dell\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "D:\Downloads\ERNIE-ernie-kit-open-v1.0_\ERNIE-ernie-kit-open-v1.0\applications\tasks\text_generation\model\ernie_infilling_generation.py", line 18, in <module>
    from erniekit.modules.ernie_gen import ErnieGenModel
  File "../../..\erniekit\modules\ernie_gen.py", line 20, in <module>
    from erniekit.modules.transformer_encoder_gen import encoder, two_stream_encoder, pre_process_layer
  File "../../..\erniekit\modules\transformer_encoder_gen.py", line 311
    out = F.dropout(out,p=dropout_rate)
                                      ^
IndentationError: unindent does not match any outer indentation level

INFO: 12-17 22:28:42: run_trainer_ernie_gen.py:91 * 12396 run trainer.... pid = 21776
INFO: 12-17 22:28:42: run_trainer_ernie_gen.py:59 * 12396 Device count: 1
INFO: 12-17 22:28:42: run_trainer_ernie_gen.py:60 * 12396 Num train examples: 15763
INFO: 12-17 22:28:42: run_trainer_ernie_gen.py:61 * 12396 Max train steps: 39407
INFO: 12-17 22:28:42: run_trainer_ernie_gen.py:62 * 12396 Num warmup steps: 3940
ERROR: 12-17 22:28:42: register.py:46 * 12396 module {key} not found: {e}
INFO: 12-17 22:29:58: params.py:43 * 7256 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:29:58: params.py:52 * 7256 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:29:58: register.py:25 * 7256 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 22:29:58: run_trainer_ernie_gen.py:91 * 7256 run trainer.... pid = 1952
INFO: 12-17 22:29:58: run_trainer_ernie_gen.py:59 * 7256 Device count: 1
INFO: 12-17 22:29:58: run_trainer_ernie_gen.py:60 * 7256 Num train examples: 15763
INFO: 12-17 22:29:58: run_trainer_ernie_gen.py:61 * 7256 Max train steps: 39407
INFO: 12-17 22:29:58: run_trainer_ernie_gen.py:62 * 7256 Num warmup steps: 3940
INFO: 12-17 22:29:58: ernie_infilling_generation.py:31 * 7256 ErnieInfillingGeneration init....
INFO: 12-17 22:29:58: static_trainer_ernie_gen.py:89 * 7256 parser meta ....
INFO: 12-17 22:29:58: static_trainer_ernie_gen.py:542 * 7256 pre_train_model's name = ernie_gen_ch
INFO: 12-17 22:29:58: static_trainer_ernie_gen.py:119 * 7256 init environment on static mode......
INFO: 12-17 22:29:58: static_trainer_ernie_gen.py:163 * 7256 gpu place....
INFO: 12-17 22:29:58: static_trainer_ernie_gen.py:344 * 7256 init_model_net.....
DEBUG: 12-17 22:29:58: ernie_gen_infilling_dataset_reader.py:145 * 7256 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 22:30:55: params.py:43 * 22840 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:30:55: params.py:52 * 22840 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:30:55: register.py:25 * 22840 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 22:30:55: run_trainer_ernie_gen.py:91 * 22840 run trainer.... pid = 19632
INFO: 12-17 22:30:55: run_trainer_ernie_gen.py:59 * 22840 Device count: 1
INFO: 12-17 22:30:55: run_trainer_ernie_gen.py:60 * 22840 Num train examples: 15763
INFO: 12-17 22:30:55: run_trainer_ernie_gen.py:61 * 22840 Max train steps: 39407
INFO: 12-17 22:30:55: run_trainer_ernie_gen.py:62 * 22840 Num warmup steps: 3940
INFO: 12-17 22:30:55: ernie_infilling_generation.py:31 * 22840 ErnieInfillingGeneration init....
INFO: 12-17 22:30:55: static_trainer_ernie_gen.py:89 * 22840 parser meta ....
INFO: 12-17 22:30:55: static_trainer_ernie_gen.py:542 * 22840 pre_train_model's name = ernie_gen_ch
INFO: 12-17 22:30:55: static_trainer_ernie_gen.py:119 * 22840 init environment on static mode......
INFO: 12-17 22:30:55: static_trainer_ernie_gen.py:163 * 22840 gpu place....
INFO: 12-17 22:30:55: static_trainer_ernie_gen.py:344 * 22840 init_model_net.....
DEBUG: 12-17 22:30:55: ernie_gen_infilling_dataset_reader.py:145 * 22840 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 22:33:35: params.py:43 * 23108 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:33:35: params.py:52 * 23108 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:33:35: register.py:25 * 23108 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 22:33:35: run_trainer_ernie_gen.py:91 * 23108 run trainer.... pid = 19520
INFO: 12-17 22:33:35: run_trainer_ernie_gen.py:59 * 23108 Device count: 1
INFO: 12-17 22:33:35: run_trainer_ernie_gen.py:60 * 23108 Num train examples: 15763
INFO: 12-17 22:33:35: run_trainer_ernie_gen.py:61 * 23108 Max train steps: 39407
INFO: 12-17 22:33:35: run_trainer_ernie_gen.py:62 * 23108 Num warmup steps: 3940
INFO: 12-17 22:33:35: ernie_infilling_generation.py:31 * 23108 ErnieInfillingGeneration init....
INFO: 12-17 22:33:35: static_trainer_ernie_gen.py:89 * 23108 parser meta ....
INFO: 12-17 22:33:35: static_trainer_ernie_gen.py:542 * 23108 pre_train_model's name = ernie_gen_ch
INFO: 12-17 22:33:35: static_trainer_ernie_gen.py:119 * 23108 init environment on static mode......
INFO: 12-17 22:33:35: static_trainer_ernie_gen.py:163 * 23108 gpu place....
INFO: 12-17 22:33:35: static_trainer_ernie_gen.py:344 * 23108 init_model_net.....
DEBUG: 12-17 22:33:35: ernie_gen_infilling_dataset_reader.py:145 * 23108 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 22:37:23: params.py:43 * 19928 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 22:37:23: params.py:52 * 19928 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 22:37:23: register.py:25 * 19928 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 22:37:23: run_trainer_ernie_gen.py:91 * 19928 run trainer.... pid = 6072
INFO: 12-17 22:37:23: run_trainer_ernie_gen.py:59 * 19928 Device count: 1
INFO: 12-17 22:37:23: run_trainer_ernie_gen.py:60 * 19928 Num train examples: 15763
INFO: 12-17 22:37:23: run_trainer_ernie_gen.py:61 * 19928 Max train steps: 39407
INFO: 12-17 22:37:23: run_trainer_ernie_gen.py:62 * 19928 Num warmup steps: 3940
INFO: 12-17 22:37:23: ernie_infilling_generation.py:31 * 19928 ErnieInfillingGeneration init....
INFO: 12-17 22:37:23: static_trainer_ernie_gen.py:89 * 19928 parser meta ....
INFO: 12-17 22:37:23: static_trainer_ernie_gen.py:542 * 19928 pre_train_model's name = ernie_gen_ch
INFO: 12-17 22:37:23: static_trainer_ernie_gen.py:119 * 19928 init environment on static mode......
INFO: 12-17 22:37:23: static_trainer_ernie_gen.py:163 * 19928 gpu place....
INFO: 12-17 22:37:23: static_trainer_ernie_gen.py:344 * 19928 init_model_net.....
DEBUG: 12-17 22:37:23: ernie_gen_infilling_dataset_reader.py:145 * 19928 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 23:02:31: params.py:43 * 18844 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 23:02:31: params.py:52 * 18844 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 23:02:31: register.py:25 * 18844 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 23:02:31: run_trainer_ernie_gen.py:91 * 18844 run trainer.... pid = 11732
INFO: 12-17 23:02:32: run_trainer_ernie_gen.py:59 * 18844 Device count: 1
INFO: 12-17 23:02:32: run_trainer_ernie_gen.py:60 * 18844 Num train examples: 15763
INFO: 12-17 23:02:32: run_trainer_ernie_gen.py:61 * 18844 Max train steps: 39407
INFO: 12-17 23:02:32: run_trainer_ernie_gen.py:62 * 18844 Num warmup steps: 3940
INFO: 12-17 23:02:32: ernie_infilling_generation.py:31 * 18844 ErnieInfillingGeneration init....
INFO: 12-17 23:02:32: static_trainer_ernie_gen.py:89 * 18844 parser meta ....
INFO: 12-17 23:02:32: static_trainer_ernie_gen.py:542 * 18844 pre_train_model's name = ernie_gen_ch
INFO: 12-17 23:02:32: static_trainer_ernie_gen.py:119 * 18844 init environment on static mode......
INFO: 12-17 23:02:32: static_trainer_ernie_gen.py:163 * 18844 gpu place....
INFO: 12-17 23:02:32: static_trainer_ernie_gen.py:344 * 18844 init_model_net.....
DEBUG: 12-17 23:02:32: ernie_gen_infilling_dataset_reader.py:145 * 18844 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
INFO: 12-17 23:33:50: params.py:43 * 23800 ./examples/cls_ernie_gen_infilling_ch.json
INFO: 12-17 23:33:50: params.py:52 * 23800 {
    "dataset_reader": {
        "dev_reader": {
            "config": {
                "batch_size": 16,
                "data_path": "./data/ernie_gen_dureader/dev.tsv",
                "epoch": 1,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": false
            },
            "fields": [],
            "name": "dev_reader",
            "type": "InfillingGenReader"
        },
        "train_reader": {
            "config": {
                "batch_size": 8,
                "data_path": "./data/ernie_gen_dureader/train.tsv",
                "epoch": 20,
                "extra_params": {
                    "do_lower_case": true,
                    "in_tokens": false,
                    "max_dec_len": 32,
                    "max_seq_len": 512,
                    "max_src_len": 320,
                    "max_tgt_len": 64,
                    "tgt_type_id": 7,
                    "tokenizer": "FullTokenizer",
                    "vocab_path": "../../models_hub/ernie_gen_ch_dir/vocab_ernie_gen_ch.txt"
                },
                "need_data_distribute": true,
                "sampling_rate": 1.0,
                "shuffle": true
            },
            "fields": [],
            "name": "train_reader",
            "type": "InfillingGenReader"
        }
    },
    "model": {
        "beam_size": 4,
        "embedding": {
            "config_path": "../../models_hub/ernie_gen_ch_dir/ernie_gen_ch_config.json",
            "emb_dim": 768,
            "use_amp": false
        },
        "is_dygraph": 0,
        "label_smooth": 0.1,
        "length_penalty": 1.0,
        "optimization": {
            "decr_every_n_nan_or_inf": 2,
            "decr_ratio": 0.8,
            "epsilon": 1e-06,
            "incr_every_n_steps": 100,
            "incr_ratio": 2.0,
            "init_loss_scaling": 128,
            "learning_rate": 5e-05,
            "lr_scheduler": "linear_warmup_decay",
            "use_default_decay": false,
            "use_dynamic_loss_scaling": false,
            "use_lr_decay": true,
            "warmup_proportion": 0.1,
            "warmup_steps": 0,
            "weight_decay": 0.01
        },
        "type": "ErnieInfillingGeneration",
        "weight_sharing": true
    },
    "trainer": {
        "PADDLE_PLACE_TYPE": "gpu",
        "eval_step": 10000,
        "is_do_train": 1,
        "is_eval_dev": 1,
        "is_eval_test": 0,
        "load_checkpoint": "",
        "load_parameters": "",
        "output_path": "./output/ernie_gen_ch",
        "pre_train_model": [
            {
                "name": "ernie_gen_ch",
                "params_path": "../../models_hub/ernie_gen_ch_dir/params"
            }
        ],
        "ramdom_seed": 1,
        "save_inference_model": true,
        "save_model_step": 100000000,
        "train_log_step": 10,
        "type": "CustomGenerationTrainer"
    }
}
WARNING: 12-17 23:33:50: register.py:25 * 23800 Key WordsegTokenizer already in registry tokenizer.
INFO: 12-17 23:33:50: run_trainer_ernie_gen.py:91 * 23800 run trainer.... pid = 5180
INFO: 12-17 23:33:51: run_trainer_ernie_gen.py:59 * 23800 Device count: 1
INFO: 12-17 23:33:51: run_trainer_ernie_gen.py:60 * 23800 Num train examples: 15763
INFO: 12-17 23:33:51: run_trainer_ernie_gen.py:61 * 23800 Max train steps: 39407
INFO: 12-17 23:33:51: run_trainer_ernie_gen.py:62 * 23800 Num warmup steps: 3940
INFO: 12-17 23:33:51: ernie_infilling_generation.py:31 * 23800 ErnieInfillingGeneration init....
INFO: 12-17 23:33:51: static_trainer_ernie_gen.py:89 * 23800 parser meta ....
INFO: 12-17 23:33:51: static_trainer_ernie_gen.py:542 * 23800 pre_train_model's name = ernie_gen_ch
INFO: 12-17 23:33:51: static_trainer_ernie_gen.py:119 * 23800 init environment on static mode......
INFO: 12-17 23:33:51: static_trainer_ernie_gen.py:163 * 23800 gpu place....
INFO: 12-17 23:33:51: static_trainer_ernie_gen.py:344 * 23800 init_model_net.....
DEBUG: 12-17 23:33:51: ernie_gen_infilling_dataset_reader.py:145 * 23800 train_reader create py_reader shape = [[-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 512], [-1, 1], [-1, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1], [-1, 512, 1024]], types = ['int64', 'int64', 'int64', 'float32', 'int64', 'int64', 'int64', 'int64', 'int64', 'float32'],                       level = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]: 
