{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 具体应用：用Paddlepaddle做一个服装广告文案生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 安装相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verify PaddlePaddle program ... \n",
      "PaddlePaddle works well on 1 GPU.\n",
      "PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n"
     ]
    }
   ],
   "source": [
    "# 检查paddlepaddle是否可以使用GPU\n",
    "import paddle\n",
    "paddle.utils.run_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照数据集要求对其进行整理，格式为“序号\\t输入文本\\t标签”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"data.xlsx\")\n",
    "Keys = df[\"关键词\"].values\n",
    "Txts = df[\"文案\"].values\n",
    "\n",
    "with open(\"format_data.txt\", \"w\",encoding='utf-8') as f:\n",
    "    for i, k in enumerate(Keys):\n",
    "        t = Txts[i]\n",
    "        f.write(\"{}\\t{}\\t{}\\n\".format(i, k, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 调用Paddlehub模型进行预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型我们选用ERNIE-GEN模型\n",
    "\n",
    "论文地址：https://arxiv.org/abs/2001.11314\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/9698f8fef8004f579d01ca0d4a8a8255d58a8cc895c44d59b97141c82a23b833)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "E:\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[32m[2023-12-30 20:08:39,263] [    INFO]\u001b[0m - Already cached C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:08:39,280] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:08:39,281] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:08:39,325] [    INFO]\u001b[0m - Already cached C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\ernie_v1_chn_base.pdparams\u001b[0m\n",
      "\u001b[35m[2023-12-30 20:08:39,327] [   DEBUG]\u001b[0m - init ErnieModel with config: {'attention_probs_dropout_prob': 0.1, 'hidden_act': 'relu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'max_position_embeddings': 513, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 18000, 'pad_token_id': 0}\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:08:39,673] [    INFO]\u001b[0m - loading pretrained model from C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\ernie_v1_chn_base.pdparams\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:08:40,196] [    INFO]\u001b[0m - param:mlm_bias not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:08:40,201] [    INFO]\u001b[0m - param:mlm.weight not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:08:40,201] [    INFO]\u001b[0m - param:mlm.bias not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:08:40,202] [    INFO]\u001b[0m - param:mlm_ln.weight not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:08:40,203] [    INFO]\u001b[0m - param:mlm_ln.bias not set in pretrained model, skip\u001b[0m\n",
      "E:\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2293: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n",
      "E:\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:1865: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2023-12-30 20:13:41,975] [    INFO]\u001b[0m - [step 100 / 1500]train loss 9.48759, ppl 13194.97949, elr 3.333e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:18:44,192] [    INFO]\u001b[0m - [step 200 / 1500]train loss 4.87576, ppl 131.07324, elr 4.815e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:23:50,157] [    INFO]\u001b[0m - [step 300 / 1500]train loss 3.63001, ppl 37.71333, elr 4.444e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:28:51,316] [    INFO]\u001b[0m - [step 400 / 1500]train loss 2.97544, ppl 19.59828, elr 4.074e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:33:55,473] [    INFO]\u001b[0m - [step 500 / 1500]train loss 2.68228, ppl 14.61838, elr 3.704e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:33:55,502] [    INFO]\u001b[0m - save the model in nlg_param\\step_500_ppl_14.61838.params\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:39:02,793] [    INFO]\u001b[0m - [step 600 / 1500]train loss 2.63281, ppl 13.91276, elr 3.333e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:44:06,740] [    INFO]\u001b[0m - [step 700 / 1500]train loss 2.37100, ppl 10.70810, elr 2.963e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:49:10,363] [    INFO]\u001b[0m - [step 800 / 1500]train loss 2.34048, ppl 10.38617, elr 2.593e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:54:14,130] [    INFO]\u001b[0m - [step 900 / 1500]train loss 2.31733, ppl 10.14856, elr 2.222e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:59:20,342] [    INFO]\u001b[0m - [step 1000 / 1500]train loss 2.35058, ppl 10.49161, elr 1.852e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 20:59:20,373] [    INFO]\u001b[0m - save the model in nlg_param\\step_1000_ppl_10.49161.params\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:04:21,267] [    INFO]\u001b[0m - [step 1100 / 1500]train loss 1.84464, ppl 6.32582, elr 1.481e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:09:26,302] [    INFO]\u001b[0m - [step 1200 / 1500]train loss 2.26187, ppl 9.60103, elr 1.111e-05\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:15:15,857] [    INFO]\u001b[0m - [step 1300 / 1500]train loss 2.18438, ppl 8.88510, elr 7.407e-06\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:20:51,873] [    INFO]\u001b[0m - [step 1400 / 1500]train loss 2.01329, ppl 7.48793, elr 3.704e-06\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:02,114] [    INFO]\u001b[0m - [step 1500 / 1500]train loss 2.00004, ppl 7.38935, elr 0.000e+00\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:02,146] [    INFO]\u001b[0m - save the model in nlg_param\\step_1500_ppl_7.38935.params\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:07,030] [    INFO]\u001b[0m - Begin export the model save in nlg_param\\step_1500_ppl_7.38935.params ...\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:09,022] [    INFO]\u001b[0m - The module has exported to C:\\Users\\11409\\Documents\\nlp\\nlg\u001b[0m\n",
      "E:\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[32m[2023-12-30 21:26:26,135] [    INFO]\u001b[0m - Successfully uninstalled nlg\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:27,649] [    INFO]\u001b[0m - Successfully installed nlg-1.0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddlehub as hub\n",
    "\n",
    "module = hub.Module(name=\"ernie_gen\")\n",
    "\n",
    "result = module.finetune(\n",
    "    train_path='format_data.txt',\n",
    "    save_dir=\"nlg_param\",\n",
    "    max_steps=1500,\n",
    "    noise_prob=0,\n",
    "    save_interval=500,\n",
    "    batch_size=16,\n",
    "    max_encode_len=100,\n",
    "    max_decode_len=100,\n",
    "    learning_rate=5e-5\n",
    ")\n",
    "\n",
    "# 将训练参数打包为hub model\n",
    "module.export(params_path=result['last_save_path'], module_name=\"nlg\", author=\"etta\")\n",
    "!hub install nlg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 运行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-12-30 21:26:29,956] [    INFO]\u001b[0m - Already cached C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\ernie_v1_chn_base.pdparams\u001b[0m\n",
      "\u001b[35m[2023-12-30 21:26:29,956] [   DEBUG]\u001b[0m - init ErnieModel with config: {'attention_probs_dropout_prob': 0.1, 'hidden_act': 'relu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'max_position_embeddings': 513, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 18000, 'pad_token_id': 0}\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:30,315] [    INFO]\u001b[0m - loading pretrained model from C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\ernie_v1_chn_base.pdparams\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:31,864] [    INFO]\u001b[0m - param:mlm_bias not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:31,864] [    INFO]\u001b[0m - param:mlm.weight not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:31,864] [    INFO]\u001b[0m - param:mlm.bias not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:31,864] [    INFO]\u001b[0m - param:mlm_ln.weight not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:31,870] [    INFO]\u001b[0m - param:mlm_ln.bias not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:32,897] [    INFO]\u001b[0m - Already cached C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:32,934] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-12-30 21:26:32,935] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddlehub as hub\n",
    "\n",
    "module = hub.Module(directory=\"nlg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这款连衣裙采用了宽松的版型设计，能够很好的修饰身材，更好的修饰身材，更好的修饰了腿部的线条，让你轻松\n",
      "这款连衣裙采用了宽松的版型设计，能够很好的修饰身材，更好的修饰身材比例，更好的修饰了腿部的线条，让你\n",
      "这款连衣裙采用了宽松的版型设计，能够很好的修饰身材，更好的修饰身材，更好的修饰了腿部的线条，更好的修\n",
      "这款连衣裙采用了宽松的版型设计，能够很好的修饰身材，更好的修饰身材比例，更好的修饰了腿部的线条，同时\n",
      "这款连衣裙采用了宽松的版型设计，能够很好的修饰身材，更好的修饰身材比例，更好的修饰了腿部的线条，更好\n",
      "这款连衣裙采用了宽松的版型设计，能够很好的修饰身材，更好的修饰身材比例，更好的修饰身材比例，更好的修\n",
      "这款连衣裙采用了宽松的版型设计，能够很好的修饰身材，更好的修饰身材比例，更好的修饰身材，更好的修饰身\n",
      "这款连衣裙采用了宽松的版型设计，能够很好的修饰腿部的线条，更好的修饰身材比例，更好的修饰身材比例，更\n",
      "这款连衣裙采用了宽松的版型设计，能够很好的修饰身材，更好的修饰身材比例，更好的修饰身材比例，更好的遮\n",
      "这款连衣裙采用了宽松的版型设计，能够很好的修饰身材，更好的修饰身材比例，更好的修饰了腿部的线条，更加\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"宽松\"]\n",
    "results = module.generate(texts=test_texts, use_gpu=False, beam_width=10)\n",
    "for result in results[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这款连衣裙采用了v领的设计，修饰脖颈线条，凸显出优美的颈部线条。精致的镂空剪裁，凸显出女性的优雅气质\n",
      "这款连衣裙采用了v领的设计，修饰脖颈线条，凸显出优美的颈部线条。精致的镂空设计，凸显出女性的优雅气质\n",
      "这款连衣裙采用了v领的设计，修饰脖颈线条，凸显出优美的颈部线条。精致的镂空剪裁，凸显出女性曼妙的身姿\n",
      "这款连衣裙采用了v领的设计，修饰脖颈线条，凸显出优美的颈部线条。精致的镂空剪裁，凸显出优雅的女人味。\n",
      "这款连衣裙采用了v领的设计，修饰脖颈线条，凸显出优美的颈部线条。精致的镂空剪裁，凸显出优雅的女人味，\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"性感\"]\n",
    "results = module.generate(texts=test_texts, use_gpu=False, beam_width=5)\n",
    "for result in results[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这款连衣裙采用优质面料，手感细腻顺滑，垂感好，垂感好，手感顺滑，垂感好，挺括有型，挺括有型，宽松的版\n",
      "这款连衣裙采用优质面料，手感细腻顺滑，垂感好，垂感好，手感顺滑，垂感好，挺括有型，挺括有型，挺括有型\n",
      "这款连衣裙采用优质面料，手感细腻顺滑，垂感好，垂感好，手感顺滑，垂感好，挺括有型，挺括有型，有型有型\n",
      "这款连衣裙采用优质面料，手感细腻顺滑，垂感好，垂感好，手感顺滑，垂感好，挺括有型，挺括有型，宽松版型\n",
      "这款连衣裙采用优质面料，手感细腻顺滑，垂感好，垂感好，手感顺滑，垂感好，挺括有型，挺括有型，宽松的廓\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"简约\"]\n",
    "results = module.generate(texts=test_texts, use_gpu=False, beam_width=5)\n",
    "for result in results[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 改进方向：\n",
    "\n",
    "可以看到数据集太少，出现了明显的过拟合现象，可以多搜集一些。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:paddle_env]",
   "language": "python",
   "name": "conda-env-paddle_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
