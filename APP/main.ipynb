{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 具体应用：用Paddlepaddle做一个凡尔赛文学生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 安装相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verify PaddlePaddle program ... \n",
      "PaddlePaddle works well on 1 GPU.\n",
      "PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n"
     ]
    }
   ],
   "source": [
    "# 检查paddlepaddle是否可以使用GPU\n",
    "import paddle\n",
    "paddle.utils.run_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照数据集要求对其进行整理，格式为“序号\\t输入文本\\t标签”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"data.xlsx\")\n",
    "Keys = df[\"关键词\"].values\n",
    "Txts = df[\"文案\"].values\n",
    "\n",
    "with open(\"format_data.txt\", \"w\",encoding='utf-8') as f:\n",
    "    for i, k in enumerate(Keys):\n",
    "        t = Txts[i]\n",
    "        # t = \"凡尔赛\"\n",
    "        f.write(\"{}\\t{}\\t{}\\n\".format(i, k, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 调用Paddlehub模型进行预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型我们选用ERNIE-GEN模型\n",
    "\n",
    "论文地址：https://arxiv.org/abs/2001.11314\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/9698f8fef8004f579d01ca0d4a8a8255d58a8cc895c44d59b97141c82a23b833)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "E:\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[32m[2023-12-19 10:49:50,008] [    INFO]\u001b[0m - Already cached C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:49:50,022] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:49:50,023] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:49:50,035] [    INFO]\u001b[0m - Already cached C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\ernie_v1_chn_base.pdparams\u001b[0m\n",
      "\u001b[35m[2023-12-19 10:49:50,037] [   DEBUG]\u001b[0m - init ErnieModel with config: {'attention_probs_dropout_prob': 0.1, 'hidden_act': 'relu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'max_position_embeddings': 513, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 18000, 'pad_token_id': 0}\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:49:50,439] [    INFO]\u001b[0m - loading pretrained model from C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\ernie_v1_chn_base.pdparams\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:49:50,977] [    INFO]\u001b[0m - param:mlm_bias not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:49:50,977] [    INFO]\u001b[0m - param:mlm.weight not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:49:50,979] [    INFO]\u001b[0m - param:mlm.bias not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:49:50,980] [    INFO]\u001b[0m - param:mlm_ln.weight not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:49:50,980] [    INFO]\u001b[0m - param:mlm_ln.bias not set in pretrained model, skip\u001b[0m\n",
      "E:\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2293: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n",
      "E:\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:1865: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[2023-12-19 10:50:26,009] [    INFO]\u001b[0m - [step 100 / 2000]train loss 11.89997, ppl 147262.62500, elr 2.500e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:50:58,286] [    INFO]\u001b[0m - [step 200 / 2000]train loss 7.40206, ppl 1639.35181, elr 5.000e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:51:30,581] [    INFO]\u001b[0m - [step 300 / 2000]train loss 5.82889, ppl 339.98203, elr 4.722e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:52:03,046] [    INFO]\u001b[0m - [step 400 / 2000]train loss 4.36250, ppl 78.45266, elr 4.444e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:52:03,049] [    INFO]\u001b[0m - save the model in Versailles_param\\step_400_ppl_78.45266.params\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:52:38,289] [    INFO]\u001b[0m - [step 500 / 2000]train loss 3.70681, ppl 40.72363, elr 4.167e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:53:10,801] [    INFO]\u001b[0m - [step 600 / 2000]train loss 3.56802, ppl 35.44638, elr 3.889e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:53:43,345] [    INFO]\u001b[0m - [step 700 / 2000]train loss 2.35687, ppl 10.55782, elr 3.611e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:54:16,080] [    INFO]\u001b[0m - [step 800 / 2000]train loss 2.62417, ppl 13.79306, elr 3.333e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:54:16,081] [    INFO]\u001b[0m - save the model in Versailles_param\\step_800_ppl_13.79306.params\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:54:50,985] [    INFO]\u001b[0m - [step 900 / 2000]train loss 1.97016, ppl 7.17179, elr 3.056e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:55:29,065] [    INFO]\u001b[0m - [step 1000 / 2000]train loss 1.66547, ppl 5.28816, elr 2.778e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:56:03,750] [    INFO]\u001b[0m - [step 1100 / 2000]train loss 1.51331, ppl 4.54175, elr 2.500e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:56:37,525] [    INFO]\u001b[0m - [step 1200 / 2000]train loss 0.93288, ppl 2.54182, elr 2.222e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:56:37,527] [    INFO]\u001b[0m - save the model in Versailles_param\\step_1200_ppl_2.54182.params\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:57:12,465] [    INFO]\u001b[0m - [step 1300 / 2000]train loss 0.88315, ppl 2.41851, elr 1.944e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:57:46,348] [    INFO]\u001b[0m - [step 1400 / 2000]train loss 1.11737, ppl 3.05681, elr 1.667e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:58:20,255] [    INFO]\u001b[0m - [step 1500 / 2000]train loss 0.81784, ppl 2.26560, elr 1.389e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:58:55,699] [    INFO]\u001b[0m - [step 1600 / 2000]train loss 0.72485, ppl 2.06442, elr 1.111e-05\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:58:55,702] [    INFO]\u001b[0m - save the model in Versailles_param\\step_1600_ppl_2.06442.params\u001b[0m\n",
      "\u001b[32m[2023-12-19 10:59:36,836] [    INFO]\u001b[0m - [step 1700 / 2000]train loss 0.96320, ppl 2.62007, elr 8.333e-06\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:00:13,091] [    INFO]\u001b[0m - [step 1800 / 2000]train loss 1.20191, ppl 3.32645, elr 5.556e-06\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:00:55,910] [    INFO]\u001b[0m - [step 1900 / 2000]train loss 0.54879, ppl 1.73116, elr 2.778e-06\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:01:40,478] [    INFO]\u001b[0m - [step 2000 / 2000]train loss 0.62693, ppl 1.87186, elr 0.000e+00\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:01:40,481] [    INFO]\u001b[0m - save the model in Versailles_param\\step_2000_ppl_1.87186.params\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:01:45,702] [    INFO]\u001b[0m - Begin export the model save in Versailles_param\\step_2000_ppl_1.87186.params ...\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:01:47,517] [    INFO]\u001b[0m - The module has exported to C:\\Users\\11409\\Documents\\nlp\\Versailles\u001b[0m\n",
      "E:\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[32m[2023-12-19 11:02:01,348] [    INFO]\u001b[0m - Successfully uninstalled Versailles\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:02:03,570] [    INFO]\u001b[0m - Successfully installed Versailles-1.0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddlehub as hub\n",
    "\n",
    "module = hub.Module(name=\"ernie_gen\")\n",
    "\n",
    "result = module.finetune(\n",
    "    train_path='format_data.txt',\n",
    "    save_dir=\"Versailles_param\",\n",
    "    max_steps=2000,\n",
    "    noise_prob=0.1,\n",
    "    save_interval=400,\n",
    "    max_encode_len=60,\n",
    "    max_decode_len=60\n",
    ")\n",
    "\n",
    "# 将训练参数打包为hub model\n",
    "module.export(params_path=result['last_save_path'], module_name=\"Versailles\", author=\"lyp\")\n",
    "!hub install Versailles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 运行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-12-19 11:02:07,560] [    INFO]\u001b[0m - Already cached C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\ernie_v1_chn_base.pdparams\u001b[0m\n",
      "\u001b[35m[2023-12-19 11:02:07,561] [   DEBUG]\u001b[0m - init ErnieModel with config: {'attention_probs_dropout_prob': 0.1, 'hidden_act': 'relu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'max_position_embeddings': 513, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 18000, 'pad_token_id': 0}\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:02:07,795] [    INFO]\u001b[0m - loading pretrained model from C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\ernie_v1_chn_base.pdparams\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:02:08,537] [    INFO]\u001b[0m - param:mlm_bias not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:02:08,539] [    INFO]\u001b[0m - param:mlm.weight not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:02:08,541] [    INFO]\u001b[0m - param:mlm.bias not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:02:08,542] [    INFO]\u001b[0m - param:mlm_ln.weight not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:02:08,543] [    INFO]\u001b[0m - param:mlm_ln.bias not set in pretrained model, skip\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:02:09,700] [    INFO]\u001b[0m - Already cached C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:02:09,733] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-12-19 11:02:09,735] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\11409\\.paddlenlp\\models\\ernie-1.0\\special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddlehub as hub\n",
    "\n",
    "module = hub.Module(directory=\"Versailles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天又是努力搬砖的一天根本没时间摸鱼中午只能休息两个小时，虽然五点就能准时下班，但早上十点就得到公司\n",
      "我在沙发上写代码，突然牛奶热好了我去拿，回来的时候发现没保存。我问他：“我写到哪了？”他不慌不忙地说\n",
      "今天又是努力搬砖的一天！根本没时间摸鱼，中午只能休息两个小时，虽然五点就能准时下班，但早上十点就得到\n",
      "我在沙发上背单词，突然牛奶热好了我去拿，回来的时候发现没保存。我问他：“我背到哪了？”他不慌不忙地说\n",
      "真羡慕那些可以随随便便离家出走的孩子，我都出来一个月了，还没走出我家草坪\n",
      "今天又是努力搬砖的一天根本没时间摸鱼，中午只能休息两个小时，虽然五点就能准时下班，但早上十点就得到公\n",
      "我在沙发上背单词，突然牛奶热好了我去拿，回来的时候发现没加书签。我问他：“我背到哪了？”他不慌不忙地\n",
      "我在沙发上背单词，突然牛奶热好了我去拿，回来的时候发现没保存。我问他：“我写到哪了？”他不慌不忙地说\n",
      "真羡慕那些可以随随便便离家出走的孩子，我都出来一个月了，还没走出我家草坪。\n",
      "天天红烧肉，顿顿女儿红，这哪是人过的日子嘛\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"凡尔赛\"]\n",
    "results = module.generate(texts=test_texts, use_gpu=False, beam_width=10)\n",
    "for result in results[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天跟朋友在tiffanycafe聊天，想说一个词死活想不出来中文怎么说，顺口就把英文说出来了，朋友说：“你有必要吗？\n",
      "今天又是努力搬砖的一天根本没时间摸鱼，中午只能休息两个小时，虽然五点就能准时下班，但早上十点就得到公\n",
      "今天又是努力搬砖的一天根本没时间摸鱼中午只能休息两个小时，虽然五点就能准时下班，但早上十点就得到公司\n",
      "今天和闺蜜逛街，柜姐还以为闺蜜是我经纪人。是因为我戴了gucci##20##52早春款墨镜吗？柜姐还说在电视上看见过我。\n",
      "今天跟朋友在tiffanycafe聊天，想说一个词死活想不出来中文怎么说，顺口就把英文说出来了，朋友说：“我有必要吗？\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"晚餐\"]\n",
    "results = module.generate(texts=test_texts, use_gpu=False, beam_width=5)\n",
    "for result in results[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哎呀凡尔赛文学到底是什么啊我现在还没搞明白，昨天刚去山里度假没信号，都没跟上时代啦！我今天早上登微博\n",
      "今天又是努力搬砖的一天根本没时间摸鱼，中午只能休息两个小时，虽然五点就能准时下班，但早上十点就得到公\n",
      "唉，真羡慕年轻人啊，活力满满见个冠军奖杯那么激动，不像我，拿好几次都没啥感觉了，上次还差点手滑给摔了\n",
      "哎呀凡尔赛文学到底是什么啊我现在还没搞明白，昨天刚去山里度假没信号，都没跟上时代啦！\n",
      "真羡慕那些可以随随便便离家出走的孩子，我都出来一个月了，还没走出我家草坪\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"首饰\"]\n",
    "results = module.generate(texts=test_texts, use_gpu=False, beam_width=5)\n",
    "for result in results[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 改进方向：\n",
    "\n",
    "可以看到数据集太少，出现了明显的过拟合现象，可以多搜集一些。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:paddle_env] *",
   "language": "python",
   "name": "conda-env-paddle_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
